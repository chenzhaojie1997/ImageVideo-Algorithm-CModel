# 超分方法的分类及优缺点
超分方法大概可以分成以下三类 \ 
1. 插值方法。最邻近插值、双线性插值、双三次插值、Lanczos等 \ 
2. 基于重建的方法。例如使用迭代反投影法、稀疏编码方法等。 \
3. 深度学习方法。例如开山之作SRCNN等

性能上看：插值 < 重建 < 深度学习 \ 
算力上看：插值 < 重建 < 深度学习 \
产品上看：重建 < 深度学习 < 插值 \

# 各方案特点
1. 插值方法实现简单、便于硬件用流水线的方式实时实现，是业界平衡算力和性能的首选方案。而且插值方法能实现任意倍率的缩放，而一个神经网络模型往往只能实现一种缩放倍率，要实现任意倍率缩放还需要在神经网络模型后端放置一个插值模型。从此角度上看，插值模块并不会因为深度学习在学界的流行而式微。\
2. 重建是一种比较尴尬的方法，论性能不如深度学习方法，论算力比插值方法更大，且硬件实现上也不一定友好。所以目前看到似乎较少有产品落地。\
3. 深度学习方案的性能很优秀，因此在学界相当流行，目前已经从CNN方案卷到了transformer方案。但是其缺点是算力要求较大，终端落地吃力。\
   3.1 深度学习模型的落地 \
   深度学习模型的实际运算速度与计算平台的算力FLOP/s、带宽Byte/s息息相关。深度学习模型的时间复杂度受限于前者，空间复杂度则受限于后者（关于这部分的知识可以参考https://zhuanlan.zhihu.com/p/34204282）。在落地神经网络的设计中，如何降低时间复杂度（把大模型砍成小模型同时保持性能等），降低空间复杂度（下采样等）是一个需要在算法模型输出阶段确定的事情，总的来说要把成本降低到可接受范围内。通过压缩等方式降数据交互次数和通过并行等手段提升带宽是后面是事情了。\
   3.2 超分模型的落地